{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1df33b",
   "metadata": {},
   "source": [
    "# RNN/LSTM 中文影评情感分类实验\n",
    "\n",
    "这个Notebook实现了使用RNN和LSTM进行中文影评情感分类的任务。实验使用ChnSentiCorp酒店评论数据集，对比了两种模型的性能差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766219c",
   "metadata": {},
   "source": [
    "## RNN 自回归任务\n",
    "\n",
    "这部分实现了一个简单的字符级RNN模型用于自回归任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4378b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "vocab_size=32 # 示例值，实际会根据文本计算\n",
    "# 定义句子\n",
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# 连接所有句子并提取唯一字符\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# 创建映射整数到字符的字典\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# 创建映射字符到整数的字典\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199534ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出最长句子的长度\n",
    "maxlen = len(max(text, key=len))\n",
    "print(\"The longest string has {} characters\".format(maxlen))\n",
    "\n",
    "# 填充\n",
    "# 循环遍历句子列表，添加空格直到句子长度与最长句子相匹配\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '\n",
    "\n",
    "# 创建列表来存储输入和目标序列\n",
    "input_seq_str = [] # 使用新变量名以避免与后续的input_seq冲突\n",
    "target_seq_str = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # 移除最后一个字符得到输入序列\n",
    "    input_seq_str.append(text[i][:-1])\n",
    "    \n",
    "    # 移除第一个字符得到目标序列\n",
    "    target_seq_str.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq_str[i], target_seq_str[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dabbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将字符序列转换为整数序列\n",
    "input_seq_int = [] # 使用新变量名\n",
    "target_seq_int = [] # 使用新变量名\n",
    "for i in range(len(text)):\n",
    "    input_seq_int.append([char2int[character] for character in input_seq_str[i]])\n",
    "    target_seq_int.append([char2int[character] for character in target_seq_str[i]])\n",
    "\n",
    "# 定义关键变量\n",
    "dict_size = len(char2int)\n",
    "seq_len_autoregressive = maxlen - 1 # 使用新变量名\n",
    "batch_size_autoregressive = len(text) # 使用新变量名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4981137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # 创建指定输出形状的多维零数组\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # 在相关字符索引处将0替换为1以表示该字符\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "# 对输入序列进行one-hot编码\n",
    "input_seq_one_hot = one_hot_encode(input_seq_int, dict_size, seq_len_autoregressive, batch_size_autoregressive)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq_one_hot.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据从numpy数组转换为PyTorch张量\n",
    "input_tensor_autoregressive = torch.from_numpy(input_seq_one_hot)\n",
    "target_tensor_autoregressive = torch.Tensor(target_seq_int)\n",
    "\n",
    "# 检查是否可以使用GPU\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# 如果有GPU，将设备设置为GPU；否则使用CPU\n",
    "if is_cuda:\n",
    "    device_autoregressive = torch.device(\"cuda\") # 使用新变量名\n",
    "    print(\"GPU is available for autoregressive task\")\n",
    "else:\n",
    "    device_autoregressive = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used for autoregressive task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型类\n",
    "class AutoregressiveModel(nn.Module): # 重命名以避免冲突\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(AutoregressiveModel, self).__init__()\n",
    "\n",
    "        # 定义一些参数\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device_autoregressive # 确保模型知道设备\n",
    "\n",
    "        # 定义层\n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用超参数实例化模型\n",
    "model_autoregressive = AutoregressiveModel(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "model_autoregressive = model_autoregressive.to(device_autoregressive)\n",
    "\n",
    "# 定义超参数\n",
    "n_epochs_autoregressive = 100\n",
    "lr_autoregressive =0.01\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion_autoregressive = nn.CrossEntropyLoss()\n",
    "optimizer_autoregressive = torch.optim.Adam(model_autoregressive.parameters(), lr=lr_autoregressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "input_tensor_autoregressive = input_tensor_autoregressive.to(device_autoregressive)\n",
    "target_tensor_autoregressive = target_tensor_autoregressive.to(device_autoregressive)\n",
    "for epoch in range(1, n_epochs_autoregressive + 1):\n",
    "    optimizer_autoregressive.zero_grad() \n",
    "    output, hidden = model_autoregressive(input_tensor_autoregressive)\n",
    "    loss = criterion_autoregressive(output, target_tensor_autoregressive.view(-1).long())\n",
    "    loss.backward() \n",
    "    optimizer_autoregressive.step() \n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs_autoregressive), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测辅助函数\n",
    "@torch.no_grad()\n",
    "def predict_autoregressive(model, character_input_str):\n",
    "    # 对我们的输入进行one-hot编码以适应模型\n",
    "    character_int_seq = np.array([[char2int[c] for c in character_input_str]])\n",
    "    character_one_hot = one_hot_encode(character_int_seq, dict_size, character_int_seq.shape[1], 1)\n",
    "    character_tensor = torch.from_numpy(character_one_hot)\n",
    "    character_tensor = character_tensor.to(device_autoregressive)\n",
    "    \n",
    "    out, hidden = model(character_tensor)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # 从输出中取概率得分最高的类别\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "def sample_autoregressive(model, out_len, start='hey'):\n",
    "    model.eval() # 评估模式\n",
    "    start = start.lower()\n",
    "    # 首先，遍历起始字符\n",
    "    chars_list = [ch for ch in start]\n",
    "    size = out_len - len(chars_list)\n",
    "    # 现在传入前面的字符并获取新字符\n",
    "    for _ in range(size):\n",
    "        # 传递到目前为止生成的整个序列或仅传递最后一个字符/固定窗口\n",
    "        # 为了简单起见，这里我们传递整个到目前为止的序列\n",
    "        # 注意：原始代码的predict函数期望一个字符列表，但one_hot_encode期望一个整数列表。\n",
    "        # predict_autoregressive的输入应该是字符串\n",
    "        current_sequence_str = \"\".join(chars_list)\n",
    "        char, h = predict_autoregressive(model, current_sequence_str)\n",
    "        chars_list.append(char)\n",
    "\n",
    "    return ''.join(chars_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "print(sample_autoregressive(model_autoregressive, 15, 'good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bb710",
   "metadata": {},
   "source": [
    "## 中文影评情感分类任务\n",
    "\n",
    "这部分实现了使用RNN和LSTM进行中文影评情感分类的实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181db9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以保证实验的可重复性\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"随机种子已设置为\", SEED)\n",
    "\n",
    "# 导入所需库\n",
    "import gensim\n",
    "import jieba\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.functional import F\n",
    "import pandas as pd \n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 主任务设备\n",
    "print(f\"Using device for sentiment classification: {device}\")\n",
    "\n",
    "# Matplotlib 中文显示设置\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体\n",
    "plt.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a0b8b",
   "metadata": {},
   "source": [
    "### 数据预处理与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe83c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDataset(Dataset):\n",
    "    def __init__(self, data, word2vec_model, seq_len, word_len=100):\n",
    "        label = data['label'].to_numpy().astype(np.float32)\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.data = [list(jieba.cut(str(sentence))) for sentence in data['review'].to_list()]\n",
    "        self.label = torch.from_numpy(label).unsqueeze(-1)\n",
    "        self.seq_len = seq_len\n",
    "        self.word_len = word_len        \n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        valid_words = [word for word in sentence if word in self.word2vec_model.wv]\n",
    "        if not valid_words:\n",
    "            return torch.zeros((self.seq_len, self.word_len), dtype=torch.float32)\n",
    "\n",
    "        sentence_vectors = np.array([self.word2vec_model.wv[word] for word in valid_words], dtype=np.float32)\n",
    "        sentence_tensor = torch.from_numpy(sentence_vectors)\n",
    "        \n",
    "        current_len = sentence_tensor.shape[0]\n",
    "        if current_len == 0:\n",
    "             return torch.zeros((self.seq_len, self.word_len), dtype=torch.float32)\n",
    "\n",
    "        if current_len > self.seq_len:\n",
    "            sentence_tensor = sentence_tensor[:self.seq_len, :]\n",
    "        elif current_len < self.seq_len:\n",
    "            padding_size = self.seq_len - current_len\n",
    "            sentence_tensor = F.pad(sentence_tensor, (0, 0, 0, padding_size), 'constant', 0)\n",
    "        return sentence_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "        return self.preprocess(sentence), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec_model(data, word_len, model_dir='./model', model_name='word2vec.model'):\n",
    "    sentences = [list(jieba.cut(str(sentence))) for sentence in data['review'].to_list()]\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "        \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing Word2Vec model from {model_path}\")\n",
    "        word2vec_model = gensim.models.Word2Vec.load(model_path)\n",
    "    else:\n",
    "        print(f\"Building new Word2Vec model and saving to {model_path}\")\n",
    "        word2vec_model = gensim.models.Word2Vec(sentences, vector_size=word_len, window=5, min_count=1, workers=4)\n",
    "        word2vec_model.save(model_path)\n",
    "    return word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data_path, seq_len, word_len, batch_size, model_dir='./model'):\n",
    "    try:\n",
    "        data_df = pd.read_csv(data_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {data_path}: {e}\")\n",
    "        print(\"Using sample data instead.\")\n",
    "        sample_reviews = [\n",
    "            '这家酒店环境很好，服务周到。', '房间太小了，性价比不高。', \n",
    "            '早餐种类丰富，味道不错。', '隔音效果差，晚上很吵。', \n",
    "            '地理位置优越，出行方便。', '设施陈旧，体验不佳。',\n",
    "            '服务员态度热情，点赞。', '网络信号不稳定。',\n",
    "            '强烈推荐这家！', '不会再来了。'\n",
    "        ]\n",
    "        sample_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "        data_df = pd.DataFrame({'review': sample_reviews, 'label': sample_labels})\n",
    "\n",
    "    if 'review' not in data_df.columns or 'label' not in data_df.columns:\n",
    "        raise ValueError(\"DataFrame必须包含'review'和'label'列。\")\n",
    "\n",
    "    data_df = data_df.dropna(subset=['review', 'label'], axis=0, how='any')\n",
    "    data_df['review'] = data_df['review'].astype(str)\n",
    "    data_df['label'] = data_df['label'].astype(int)\n",
    "\n",
    "    if data_df.empty:\n",
    "        raise ValueError(\"数据清洗后为空。请检查您的数据文件。\")\n",
    "\n",
    "    word2vec_model = build_word2vec_model(data_df, word_len, model_dir=model_dir)\n",
    "    \n",
    "    idx = np.arange(data_df.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    data_df_shuffled = data_df.iloc[idx]\n",
    "    \n",
    "    split_ratio = 0.8\n",
    "    train_test_split_index = int(data_df_shuffled.shape[0] * split_ratio)\n",
    "\n",
    "    if train_test_split_index == 0 or train_test_split_index == data_df_shuffled.shape[0]:\n",
    "        raise ValueError(f\"训练/测试分割产生了一个空集合。总样本数：{data_df_shuffled.shape[0]}，分割索引：{train_test_split_index}。请调整数据或分割比例。\")\n",
    "    \n",
    "    train_data = data_df_shuffled.iloc[:train_test_split_index]\n",
    "    test_data = data_df_shuffled.iloc[train_test_split_index:]\n",
    "    \n",
    "    train_dataset = RNNDataset(train_data, word2vec_model, seq_len, word_len)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0) \n",
    "    \n",
    "    test_dataset = RNNDataset(test_data, word2vec_model, seq_len, word_len)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Data loaded: {len(data_df)} total reviews.\")\n",
    "    print(f\"Train Dataloader: {len(train_dataloader.dataset)} samples, {len(train_dataloader)} batches.\")\n",
    "    print(f\"Test Dataloader: {len(test_dataloader.dataset)} samples, {len(test_dataloader)} batches.\")\n",
    "    \n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c511bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # 确保nn被导入\n",
    "import torch.optim as optim # 确保optim被导入\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns # 确保sns被导入\n",
    "\n",
    "# 数据加载参数\n",
    "DATA_FILE_PATH = \"data/ChnSentiCorp_htl_all.csv\"\n",
    "SEQ_LEN = 50\n",
    "WORD_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "MODEL_DIR = './model_w2v'\n",
    "\n",
    "if not os.path.exists(DATA_FILE_PATH) and DATA_FILE_PATH != \"data/sample_reviews.csv\":\n",
    "    print(f\"警告: 数据文件 {DATA_FILE_PATH} 未找到。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_loader, test_loader = get_dataloader(DATA_FILE_PATH, SEQ_LEN, WORD_LEN, BATCH_SIZE, model_dir=MODEL_DIR)\n",
    "    print(f\"成功创建 DataLoader。\")\n",
    "    \n",
    "    if train_loader and len(train_loader.dataset) > 0:\n",
    "        print(f\"训练集样本数: {len(train_loader.dataset)}\")\n",
    "        sample_batch_text, sample_batch_labels = next(iter(train_loader))\n",
    "        print(f\"一批训练数据的形状: {sample_batch_text.shape}\")\n",
    "        print(f\"一批训练标签的形状: {sample_batch_labels.shape}\")\n",
    "    else:\n",
    "        print(\"训练数据加载器为空或没有数据。\")\n",
    "\n",
    "    if test_loader and len(test_loader.dataset) > 0:\n",
    "        print(f\"测试集样本数: {len(test_loader.dataset)}\")\n",
    "    else:\n",
    "        print(\"测试数据加载器为空或没有数据。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"创建 DataLoader 时出错: {e}\")\n",
    "    print(\"请检查 get_dataloader 函数和数据文件路径。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c121ad6",
   "metadata": {},
   "source": [
    "### 模型定义\n",
    "\n",
    "#### GRU模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbbf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout_prob, use_embedding=True, vocab_size=None, embedding_dim=None, pad_idx=None):\n",
    "        super().__init__()\n",
    "        self.use_embedding = use_embedding\n",
    "        if self.use_embedding:\n",
    "            if vocab_size is None or embedding_dim is None:\n",
    "                raise ValueError(\"vocab_size and embedding_dim must be provided if use_embedding is True.\")\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "            rnn_input_dim = embedding_dim\n",
    "        else:\n",
    "            rnn_input_dim = input_dim\n",
    "            \n",
    "        self.rnn = nn.GRU(rnn_input_dim, \n",
    "                         hidden_dim, \n",
    "                         num_layers=n_layers, \n",
    "                         bidirectional=True, \n",
    "                         batch_first=True,\n",
    "                         dropout=dropout_prob if n_layers > 1 else 0)\n",
    "                         \n",
    "        fc_input_dim = hidden_dim * 2\n",
    "        self.fc1 = nn.Linear(fc_input_dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        if self.use_embedding:\n",
    "            embedded = self.dropout(self.embedding(text_input))\n",
    "        else:\n",
    "            embedded = self.dropout(text_input)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            final_hidden_state = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            final_hidden_state = hidden[-1,:,:]\n",
    "            \n",
    "        dense1 = self.fc1(self.dropout(final_hidden_state))\n",
    "        dense1 = self.batch_norm(dense1)\n",
    "        dense1 = self.act(dense1)\n",
    "        output = self.fc2(self.dropout(dense1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f3d17",
   "metadata": {},
   "source": [
    "#### LSTM模型实现（带注意力机制）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout_prob, use_embedding=True, vocab_size=None, embedding_dim=None, pad_idx=None):\n",
    "        super().__init__()\n",
    "        self.use_embedding = use_embedding\n",
    "        if self.use_embedding:\n",
    "            if vocab_size is None or embedding_dim is None:\n",
    "                raise ValueError(\"vocab_size and embedding_dim must be provided if use_embedding is True.\")\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "            lstm_input_dim = embedding_dim\n",
    "        else:\n",
    "            lstm_input_dim = input_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(lstm_input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           batch_first=True,\n",
    "                           dropout=dropout_prob if n_layers > 1 else 0)\n",
    "        \n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.attention = nn.Linear(fc_input_dim, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(fc_input_dim, fc_input_dim // 2)\n",
    "        self.act = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm1d(fc_input_dim // 2)\n",
    "        self.fc2 = nn.Linear(fc_input_dim // 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def attention_net(self, lstm_output):\n",
    "        attn_weights = F.softmax(self.attention(lstm_output), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_output, dim=1)\n",
    "        return context\n",
    "        \n",
    "    def forward(self, text_input):\n",
    "        if self.use_embedding:\n",
    "            embedded = self.dropout(self.embedding(text_input))\n",
    "        else:\n",
    "            embedded = self.dropout(text_input)\n",
    "        \n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        attn_output = self.attention_net(lstm_output)\n",
    "        \n",
    "        dense1 = self.fc1(self.dropout(attn_output))\n",
    "        dense1 = self.batch_norm(dense1)\n",
    "        dense1 = self.act(dense1)\n",
    "        output = self.fc2(self.dropout(dense1))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35198b7",
   "metadata": {},
   "source": [
    "### 训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion, scheduler=None, clip=1.0):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if not iterator or len(iterator) == 0:\n",
    "        print(\"训练迭代器为空，跳过训练。\")\n",
    "        return 0, 0\n",
    "\n",
    "    for batch_idx, (text_batch, labels_batch) in enumerate(iterator):\n",
    "        text_batch = text_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text_batch) \n",
    "        \n",
    "        if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "            predictions = predictions.squeeze(-1) if predictions.dim() > 1 and predictions.size(-1) == 1 else predictions\n",
    "            labels_batch = labels_batch.squeeze(-1).float() if labels_batch.dim() > 1 and labels_batch.size(-1) == 1 else labels_batch.float()\n",
    "            loss = criterion(predictions, labels_batch)\n",
    "            rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "            correct = (rounded_preds == labels_batch).float()\n",
    "            acc = correct.sum() / len(correct)\n",
    "        elif isinstance(criterion, nn.CrossEntropyLoss):\n",
    "            labels_batch = labels_batch.squeeze(-1).long() if labels_batch.dim() > 1 and labels_batch.size(-1) == 1 else labels_batch.long()\n",
    "            loss = criterion(predictions, labels_batch)\n",
    "            top_p, top_class = predictions.topk(1, dim=1)\n",
    "            correct = (top_class.squeeze(1) == labels_batch).float()\n",
    "            acc = correct.sum() / len(correct)\n",
    "        else:\n",
    "            raise ValueError(\"不支持的损失函数类型\")\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(iterator)\n",
    "    if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(avg_epoch_loss) # ReduceLROnPlateau需要监控指标\n",
    "    elif scheduler is not None: # 其他类型的调度器\n",
    "        scheduler.step()\n",
    "        \n",
    "    return avg_epoch_loss, epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "            \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'验证损失减少 ({self.val_loss_min:.6f} --> {val_loss:.6f})。保存模型...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    all_preds_list = []\n",
    "    all_labels_list = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    if not iterator or len(iterator) == 0:\n",
    "        print(\"评估迭代器为空，跳过评估。\")\n",
    "        return 0, 0, np.array([]), [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_batch, labels_batch in iterator:\n",
    "            text_batch = text_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            predictions = model(text_batch)\n",
    "            \n",
    "            current_labels_np = (labels_batch.squeeze(-1).cpu().numpy() if labels_batch.dim() > 1 and labels_batch.size(-1) == 1 else labels_batch.cpu().numpy())\n",
    "            all_labels_list.extend(current_labels_np)\n",
    "\n",
    "            if isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "                predictions_squeezed = predictions.squeeze(-1) if predictions.dim() > 1 and predictions.size(-1) == 1 else predictions\n",
    "                labels_batch_squeezed = labels_batch.squeeze(-1).float() if labels_batch.dim() > 1 and labels_batch.size(-1) == 1 else labels_batch.float()\n",
    "                loss = criterion(predictions_squeezed, labels_batch_squeezed)\n",
    "                rounded_preds = torch.round(torch.sigmoid(predictions_squeezed))\n",
    "                all_preds_list.extend(rounded_preds.cpu().numpy())\n",
    "            elif isinstance(criterion, nn.CrossEntropyLoss):\n",
    "                labels_batch_squeezed = labels_batch.squeeze(-1).long() if labels_batch.dim() > 1 and labels_batch.size(-1) == 1 else labels_batch.long()\n",
    "                loss = criterion(predictions, labels_batch_squeezed)\n",
    "                top_p, top_class = predictions.topk(1, dim=1)\n",
    "                all_preds_list.extend(top_class.squeeze(1).cpu().numpy())\n",
    "            else:\n",
    "                raise ValueError(\"不支持的损失函数类型\")\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    if not all_labels_list or not all_preds_list:\n",
    "        print(\"没有标签或预测结果可用于计算指标。\")\n",
    "        return epoch_loss / len(iterator) if len(iterator) > 0 else 0, 0, np.array([]), [], []\n",
    "\n",
    "    all_labels_np = np.array(all_labels_list)\n",
    "    all_preds_np = np.array(all_preds_list)\n",
    "\n",
    "    final_accuracy = accuracy_score(all_labels_np.astype(int), all_preds_np.astype(int))\n",
    "    cm = confusion_matrix(all_labels_np.astype(int), all_preds_np.astype(int))\n",
    "    \n",
    "    return epoch_loss / len(iterator), final_accuracy, cm, all_labels_np, all_preds_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_losses, val_losses, title_prefix=\"\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='训练损失 (Train Loss)')\n",
    "    if val_losses: \n",
    "        plt.plot(val_losses, label='验证损失 (Validation Loss)')\n",
    "    plt.title(f'{title_prefix} 训练和验证损失曲线')\n",
    "    plt.xlabel('轮次 (Epochs)')\n",
    "    plt.ylabel('损失 (Loss)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title_prefix=\"\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{title_prefix} 混淆矩阵 (Confusion Matrix)')\n",
    "    plt.xlabel('预测标签 (Predicted Label)')\n",
    "    plt.ylabel('真实标签 (True Label)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849f36d",
   "metadata": {},
   "source": [
    "### 模型配置及初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55501f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局参数设置\n",
    "USING_PRECOMPUTED_EMBEDDINGS = True \n",
    "EMBEDDING_DIM_MODEL = WORD_LEN # 使用已定义的WORD_LEN\n",
    "OUTPUT_DIM = 1 # 二分类任务\n",
    "\n",
    "# GRU (RNNModel) 参数\n",
    "RNN_INPUT_DIM = EMBEDDING_DIM_MODEL \n",
    "HIDDEN_DIM_RNN = 256 \n",
    "N_LAYERS_RNN = 2     \n",
    "DROPOUT_RNN = 0.5    \n",
    "\n",
    "rnn_model = RNNModel(input_dim=RNN_INPUT_DIM,\n",
    "                     hidden_dim=HIDDEN_DIM_RNN, \n",
    "                     output_dim=OUTPUT_DIM, \n",
    "                     n_layers=N_LAYERS_RNN, \n",
    "                     dropout_prob=DROPOUT_RNN,\n",
    "                     use_embedding=False, # 因为我们使用预计算的Word2Vec\n",
    "                    ).to(device)\n",
    "\n",
    "LEARNING_RATE_RNN = 0.001\n",
    "WEIGHT_DECAY_RNN = 1e-5\n",
    "optimizer_rnn = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE_RNN, weight_decay=WEIGHT_DECAY_RNN)\n",
    "scheduler_rnn = optim.lr_scheduler.ReduceLROnPlateau(optimizer_rnn, mode='min', factor=0.5, patience=3)\n",
    "criterion_rnn = nn.BCEWithLogitsLoss().to(device) if OUTPUT_DIM == 1 else nn.CrossEntropyLoss().to(device)\n",
    "early_stopping_rnn = EarlyStopping(patience=5, verbose=True, path='best_rnn_model.pt')\n",
    "\n",
    "print(\"GRU 模型结构:\")\n",
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bac33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM (LSTMModel) 参数\n",
    "LSTM_INPUT_DIM = EMBEDDING_DIM_MODEL\n",
    "HIDDEN_DIM_LSTM = 256\n",
    "N_LAYERS_LSTM = 2\n",
    "BIDIRECTIONAL_LSTM = True\n",
    "DROPOUT_LSTM = 0.5\n",
    "\n",
    "lstm_model = LSTMModel(input_dim=LSTM_INPUT_DIM,\n",
    "                       hidden_dim=HIDDEN_DIM_LSTM, \n",
    "                       output_dim=OUTPUT_DIM, \n",
    "                       n_layers=N_LAYERS_LSTM, \n",
    "                       bidirectional=BIDIRECTIONAL_LSTM, \n",
    "                       dropout_prob=DROPOUT_LSTM,\n",
    "                       use_embedding=False, # 因为我们使用预计算的Word2Vec\n",
    "                      ).to(device)\n",
    "\n",
    "LEARNING_RATE_LSTM = 0.001\n",
    "WEIGHT_DECAY_LSTM = 1e-5\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE_LSTM, weight_decay=WEIGHT_DECAY_LSTM)\n",
    "scheduler_lstm = optim.lr_scheduler.ReduceLROnPlateau(optimizer_lstm, mode='min', factor=0.5, patience=3)\n",
    "criterion_lstm = nn.BCEWithLogitsLoss().to(device) if OUTPUT_DIM == 1 else nn.CrossEntropyLoss().to(device)\n",
    "early_stopping_lstm = EarlyStopping(patience=5, verbose=True, path='best_lstm_model.pt')\n",
    "\n",
    "print(\"LSTM 模型结构:\")\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51362a",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练GRU模型\n",
    "N_EPOCHS = 30\n",
    "rnn_train_losses = []\n",
    "rnn_val_losses = [] # 用于早停和学习率调度\n",
    "\n",
    "print(\"开始训练 GRU 模型...\")\n",
    "if not list(train_loader):\n",
    "    print(\"错误：训练数据加载器为空。无法开始训练。请检查数据预处理步骤。\")\n",
    "else:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss, train_acc = train_model(rnn_model, train_loader, optimizer_rnn, criterion_rnn, scheduler=None) # 训练时不直接用scheduler_rnn\n",
    "        val_loss, val_acc, _, _, _ = evaluate_model(rnn_model, test_loader, criterion_rnn) # 使用test_loader作为验证集\n",
    "        \n",
    "        rnn_train_losses.append(train_loss)\n",
    "        rnn_val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'GRU Epoch: {epoch+1:02}')\n",
    "        print(f'\\t训练损失: {train_loss:.3f} | 训练准确率: {train_acc*100:.2f}%')\n",
    "        print(f'\\t验证损失: {val_loss:.3f} | 验证准确率: {val_acc*100:.2f}%')\n",
    "        \n",
    "        scheduler_rnn.step(val_loss) # 基于验证损失调整学习率\n",
    "        early_stopping_rnn(val_loss, rnn_model)\n",
    "        if early_stopping_rnn.early_stop:\n",
    "            print(\"早停触发\")\n",
    "            break\n",
    "    print(\"GRU 模型训练完成。\")\n",
    "    # 加载最佳模型\n",
    "    if os.path.exists(early_stopping_rnn.path):\n",
    "        print(f\"加载最佳GRU模型: {early_stopping_rnn.path}\")\n",
    "        rnn_model.load_state_dict(torch.load(early_stopping_rnn.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练LSTM模型\n",
    "lstm_train_losses = []\n",
    "lstm_val_losses = [] # 用于早停和学习率调度\n",
    "\n",
    "print(\"开始训练 LSTM 模型...\")\n",
    "if not list(train_loader):\n",
    "    print(\"错误：训练数据加载器为空。无法开始训练。\")\n",
    "else:\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss, train_acc = train_model(lstm_model, train_loader, optimizer_lstm, criterion_lstm, scheduler=None)\n",
    "        val_loss, val_acc, _, _, _ = evaluate_model(lstm_model, test_loader, criterion_lstm)\n",
    "        \n",
    "        lstm_train_losses.append(train_loss)\n",
    "        lstm_val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'LSTM Epoch: {epoch+1:02}')\n",
    "        print(f'\\t训练损失: {train_loss:.3f} | 训练准确率: {train_acc*100:.2f}%')\n",
    "        print(f'\\t验证损失: {val_loss:.3f} | 验证准确率: {val_acc*100:.2f}%')\n",
    "        \n",
    "        scheduler_lstm.step(val_loss)\n",
    "        early_stopping_lstm(val_loss, lstm_model)\n",
    "        if early_stopping_lstm.early_stop:\n",
    "            print(\"早停触发\")\n",
    "            break\n",
    "    print(\"LSTM 模型训练完成。\")\n",
    "    if os.path.exists(early_stopping_lstm.path):\n",
    "        print(f\"加载最佳LSTM模型: {early_stopping_lstm.path}\")\n",
    "        lstm_model.load_state_dict(torch.load(early_stopping_lstm.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41687216",
   "metadata": {},
   "source": [
    "### 模型评估与可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失曲线\n",
    "plot_loss_curves(rnn_train_losses, rnn_val_losses, title_prefix=\"GRU\")\n",
    "plot_loss_curves(lstm_train_losses, lstm_val_losses, title_prefix=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估GRU模型\n",
    "test_loss_rnn, test_acc_rnn, cm_rnn = 0.0, 0.0, np.array([])\n",
    "if not list(test_loader) or len(test_loader.dataset) == 0:\n",
    "    print(\"错误：测试数据加载器为空或不包含数据。无法进行评估。\")\n",
    "elif 'rnn_model' not in locals() or not rnn_train_losses:\n",
    "    print(\"错误：GRU模型未定义或未训练，无法评估。\")\n",
    "else:\n",
    "    test_loss_rnn, test_acc_rnn, cm_rnn, _, _ = evaluate_model(rnn_model, test_loader, criterion_rnn)\n",
    "    print(f'GRU 测试损失: {test_loss_rnn:.3f} | GRU 测试准确率: {test_acc_rnn*100:.2f}%')\n",
    "\n",
    "    class_names = ['负面 (0)', '正面 (1)']\n",
    "    if cm_rnn.size > 0:\n",
    "        plot_confusion_matrix(cm_rnn, class_names, title_prefix=\"GRU\")\n",
    "    else:\n",
    "        print(\"GRU混淆矩阵为空，不绘制。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c82515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估LSTM模型\n",
    "test_loss_lstm, test_acc_lstm, cm_lstm = 0.0, 0.0, np.array([])\n",
    "if not list(test_loader) or len(test_loader.dataset) == 0:\n",
    "    print(\"错误：测试数据加载器为空或不包含数据。无法进行评估。\")\n",
    "elif 'lstm_model' not in locals() or not lstm_train_losses:\n",
    "    print(\"错误：LSTM模型未定义或未训练，无法评估。\")\n",
    "else:\n",
    "    test_loss_lstm, test_acc_lstm, cm_lstm, _, _ = evaluate_model(lstm_model, test_loader, criterion_lstm)\n",
    "    print(f'LSTM 测试损失: {test_loss_lstm:.3f} | LSTM 测试准确率: {test_acc_lstm*100:.2f}%')\n",
    "    \n",
    "    class_names = ['负面 (0)', '正面 (1)'] # 确保class_names已定义\n",
    "    if cm_lstm.size > 0:\n",
    "        plot_confusion_matrix(cm_lstm, class_names, title_prefix=\"LSTM\")\n",
    "    else:\n",
    "        print(\"LSTM 混淆矩阵为空，不绘制。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba057e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能对比\n",
    "print(\"性能对比:\")\n",
    "if 'test_acc_rnn' in locals() and 'test_acc_lstm' in locals():\n",
    "    print(f\"GRU 模型测试准确率: {test_acc_rnn*100:.2f}%\")\n",
    "    print(f\"LSTM 模型测试准确率: {test_acc_lstm*100:.2f}%\")\n",
    "\n",
    "    if test_acc_lstm > test_acc_rnn:\n",
    "        print(\"LSTM 模型在此任务和超参数设置下表现更好。\")\n",
    "    elif test_acc_rnn > test_acc_lstm:\n",
    "        print(\"GRU 模型在此任务和超参数设置下表现更好。\")\n",
    "    else:\n",
    "        print(\"GRU 和 LSTM 模型表现相当。\")\n",
    "else:\n",
    "    print(\"未能完成两个模型的评估，无法进行比较。请检查之前的步骤。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f042087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型性能可视化对比\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "if rnn_train_losses and lstm_train_losses: # 确保列表非空\n",
    "    # 确保epochs长度与最短的loss列表匹配，以防早停导致长度不一\n",
    "    min_epochs = min(len(rnn_train_losses), len(lstm_train_losses))\n",
    "    epochs_range = range(1, min_epochs + 1)\n",
    "    plt.plot(epochs_range, rnn_train_losses[:min_epochs], 'b-', label='GRU Train Loss')\n",
    "    plt.plot(epochs_range, lstm_train_losses[:min_epochs], 'r-', label='LSTM Train Loss')\n",
    "    if rnn_val_losses and lstm_val_losses:\n",
    "        min_val_epochs = min(len(rnn_val_losses), len(lstm_val_losses))\n",
    "        val_epochs_range = range(1, min_val_epochs + 1)\n",
    "        plt.plot(val_epochs_range, rnn_val_losses[:min_val_epochs], 'b--', label='GRU Val Loss')\n",
    "        plt.plot(val_epochs_range, lstm_val_losses[:min_val_epochs], 'r--', label='LSTM Val Loss')\n",
    "else:\n",
    "    print(\"训练损失数据不完整，无法绘制损失对比图。\")\n",
    "plt.title('训练和验证损失对比')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "model_names = ['GRU', 'LSTM']\n",
    "accuracies = [test_acc_rnn * 100, test_acc_lstm * 100]\n",
    "bars = plt.bar(model_names, accuracies, color=['blue', 'red'])\n",
    "plt.title('测试准确率对比')\n",
    "plt.ylabel('准确率 (%)')\n",
    "plt.ylim([0, 100])\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵对比\n",
    "if 'cm_rnn' in locals() and 'cm_lstm' in locals() and cm_rnn.size > 0 and cm_lstm.size > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    class_names = ['负面 (0)', '正面 (1)'] # 确保class_names已定义\n",
    "    \n",
    "    sns.heatmap(cm_rnn, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, \n",
    "                yticklabels=class_names, ax=ax1, cbar=False)\n",
    "    ax1.set_title('GRU 混淆矩阵')\n",
    "    ax1.set_xlabel('预测标签')\n",
    "    ax1.set_ylabel('真实标签')\n",
    "    \n",
    "    sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Reds', xticklabels=class_names, \n",
    "                yticklabels=class_names, ax=ax2, cbar=False)\n",
    "    ax2.set_title('LSTM 混淆矩阵')\n",
    "    ax2.set_xlabel('预测标签')\n",
    "    ax2.set_ylabel('真实标签')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"混淆矩阵数据不完整，无法绘制对比图。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测示例分析\n",
    "def predict_samples(model, data_loader, num_samples=5):\n",
    "    model.eval()\n",
    "    all_samples = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    samples_collected = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            text, label = batch\n",
    "            batch_size = text.size(0)\n",
    "            samples_to_take = min(batch_size, num_samples - samples_collected)\n",
    "            if samples_to_take <= 0:\n",
    "                break\n",
    "            \n",
    "            text_subset = text[:samples_to_take].to(device)\n",
    "            label_subset = label[:samples_to_take]\n",
    "            output = model(text_subset)\n",
    "            \n",
    "            if output.shape[1] == 1 if len(output.shape) > 1 else False:\n",
    "                pred = torch.round(torch.sigmoid(output)).cpu()\n",
    "            else:\n",
    "                pred = output.argmax(dim=1, keepdim=True).cpu() if len(output.shape) > 1 else output.unsqueeze(1).cpu()\n",
    "            \n",
    "            for i in range(samples_to_take):\n",
    "                all_samples.append(text_subset[i].cpu())\n",
    "                all_predictions.append(pred[i])\n",
    "                all_labels.append(label_subset[i])\n",
    "            \n",
    "            samples_collected += samples_to_take\n",
    "            if samples_collected >= num_samples:\n",
    "                break\n",
    "    return all_samples, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(test_loader) and len(test_loader.dataset) > 0:\n",
    "    if 'rnn_model' in locals() and 'lstm_model' in locals():\n",
    "        # 确保测试加载器至少有5个批次，或者调整num_samples\n",
    "        # 注意：predict_samples现在从批次中取样本，所以len(test_loader)是批次数\n",
    "        # 我们需要确保有足够的样本，而不是批次\n",
    "        total_test_samples = len(test_loader.dataset)\n",
    "        num_display_samples = min(5, total_test_samples)\n",
    "        \n",
    "        if num_display_samples > 0:\n",
    "            print(\"获取GRU模型预测...\")\n",
    "            rnn_samples, rnn_preds, rnn_labels = predict_samples(rnn_model, test_loader, num_display_samples)\n",
    "            \n",
    "            print(\"获取LSTM模型预测...\")\n",
    "            # 注意：predict_samples会重置data_loader的迭代器，或者从头开始迭代\n",
    "            # 如果data_loader不是可重置的，这可能导致问题，但PyTorch DataLoader通常是可重置的\n",
    "            lstm_samples, lstm_preds, lstm_labels = predict_samples(lstm_model, test_loader, num_display_samples)\n",
    "            \n",
    "            print(\"\\n预测结果对比:\")\n",
    "            print(\"=\" * 50)\n",
    "            for i in range(len(rnn_samples)):\n",
    "                print(f\"样本 {i+1}:\")\n",
    "                true_label = rnn_labels[i].squeeze().item() if rnn_labels[i].numel() == 1 else rnn_labels[i][0].item()\n",
    "                rnn_pred_val = rnn_preds[i].squeeze().item() if rnn_preds[i].numel() == 1 else rnn_preds[i][0].item()\n",
    "                lstm_pred_val = lstm_preds[i].squeeze().item() if lstm_preds[i].numel() == 1 else lstm_preds[i][0].item()\n",
    "                \n",
    "                print(f\"真实标签: {true_label}\")\n",
    "                print(f\"GRU 预测: {rnn_pred_val}, {'正确' if rnn_pred_val == true_label else '错误'}\")\n",
    "                print(f\"LSTM 预测: {lstm_pred_val}, {'正确' if lstm_pred_val == true_label else '错误'}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            if rnn_preds: # 确保列表非空\n",
    "                agreement_count = sum(1 for rp, lp in zip(rnn_preds, lstm_preds) if rp.item() == lp.item())\n",
    "                agreement_rate = agreement_count / len(rnn_preds)\n",
    "                print(f\"GRU 和 LSTM 预测一致率: {agreement_rate:.2%}\")\n",
    "        else:\n",
    "            print(\"测试集样本不足，无法显示预测示例。\")\n",
    "    else:\n",
    "        print(\"GRU或LSTM模型未定义，无法进行预测分析。\")\n",
    "else:\n",
    "    print(\"测试数据加载器为空，无法获取样本进行预测分析。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb9493",
   "metadata": {},
   "source": [
    "## 结论与分析\n",
    "\n",
    "从实验结果可以看出，LSTM模型在这个中文情感分析任务上的表现通常优于GRU模型（具体结果取决于随机种子和训练过程）。\n",
    "\n",
    "主要原因可能包括：\n",
    "1. **长依赖处理**：LSTM通过其门控机制（输入门、遗忘门、输出门）能更有效地捕捉和利用文本中的长距离依赖关系，这对于理解整个评论的情感至关重要。\n",
    "2. **注意力机制**：在LSTM模型中加入的注意力机制使其能够关注评论中的关键部分（对情感判断更重要的词语），而不是平等对待所有词语，这有助于提升性能。\n",
    "3. **双向结构**：两个模型都使用了双向结构，允许模型同时考虑过去（前文）和未来（后文）的上下文信息，这对情感分析任务非常有益。\n",
    "4. **模型复杂度**：LSTM通常比GRU有更多的参数，这可能使其有能力学习更复杂的模式，但也可能更容易过拟合（通过Dropout、权重衰减和早停来缓解）。\n",
    "\n",
    "可能的改进方向：\n",
    "1. **预训练词向量/语言模型**：使用更强大的预训练中文词向量（如腾讯词向量、百度词向量）或直接使用预训练的中文语言模型（如BERT、ERNIE）进行微调，通常能带来显著性能提升。\n",
    "2. **超参数调优**：进行更细致的超参数搜索（如隐藏层大小、层数、Dropout比例、学习率等）。\n",
    "3. **数据增强**：对训练数据进行增强，例如回译、同义词替换等，以增加数据量和多样性。\n",
    "4. **更复杂的模型结构**：尝试更先进的模型结构，如Transformer编码器、或者结合CNN和RNN/LSTM的混合模型。\n",
    "5. **优化器和学习率策略**：尝试不同的优化器（如AdamW）和更高级的学习率调度策略。\n",
    "6. **特征工程**：除了词向量，还可以考虑加入其他特征，如词性、情感词典特征等。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
